Resource allocation in multi-camera systems is a key challenge. Automatic target tracking in such
systems requires to use efficiently available assets while being constrained by uncertainty in
measurements and partial environment observability. Such constraints are common in the real world,
and automating these and similar tasks can have a significant impact in many fields of application.
These problems can be modeled mathematically using a framework called Partially Observable Markov
Decision Process (POMDPs), where a probabilistic automaton models an agent capable of exerting
influence on the world around it. There is extensive research on planning algorithms on POMDPs where
the planning agent behavior is determined by rewards obtained by acting within the environment and
reaching predefined objectives. This work improves upon existing methods in order to apply them to
problems where goals are information-driven; i.e. the agent is rewarded for collecting information
regardless of the state of the environment. Our algorithm, $\rho$POMCP, can be used to efficiently
collect information towards the minimization of either entropy or max-of-belief information
functions. By leveraging Monte Carlo techniques and other approximations our approach is applicable
to problems orders of magnitude bigger than existing alternatives and can be generally applied to
all information dependent tasks, even when objectives are not fully information driven. Our approach
is compared against already existing methods, showing that $\rho$POMCP can provide results as good
as existing methods in a fraction of the time, demonstrating high performance and scalability.
