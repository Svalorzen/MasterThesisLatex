%\item multi-sensor systems are everywhere. ex ....

With the advent of cheap hardware, these last few years have brought an incredible widespread
adoption of multi-camera systems. These systems are used for surveillance, real-time tracking and
many other purposes. Deployment expenses of such systems have progressively decreased with the
advent of digital multiplexing, the Internet and new manufacturing techniques. At the same time the
cost for operating the cameras and storing/analyzing the incredible amount of data they provide has
been increasing ever since. This often results in a need to operate multi-camera systems within
tight constraints, caused by both budget and technological limitations. Some such constraints can
be:

%\item they have resource constraints sometimes , ex ....

\begin{description}

\item[Limited Sensor Range:] Generally it is impossible for sensors to perceive simultaneously the
    whole environment in which they are located. This can result from a limited number of available
    sensors and/or from sensors having physical limits in what they can record.

% \ys{limited sensor range, how is it a resource constraint which can be manipulated?}
%
% Eugenio: I don't understand. I never really said that these constraints are going to be
% manipulated. Why would that be? It's just a constraint, which we need to overcome. We need to act
% in order to compensate for the fact that we cannot see everything all at once - the reasoning with
% uncertainty part. I also cannot see how the other constraints are manipulated.. It's not like we can
% "manipulate" a bandwidth restraint. We can act so that we can work within its bounds, but the
% constraint itself never changes. We don't really manipulate it.

\ys{Yes you are right that we are trying to work within the bounds presented by the constraints. But my point there was that the thesis is addressing an issue which deals with enery, bandwidth or communication constraints. I dont particular mind if you want to give examples of other constraints. But I think other people might point that why was a certain resource constraint even discussed, when it was not relevant to thesis's address. This becomes more prominent when you present it a big bullet point. I would rather elaborate more on how bandwidth, enery of comunication can be a resource constraint.}

\item[Energy, Bandwidth and Communication Restraints:] Gathering and processing information consumes
    resources. Not only energy and bandwidth are expensive, but in many applications there are hard
    limits on their availability, both for what can be used at any given time and for the overall
    resource usage.

\item[Environment Size:] Camera systems are often deployed in order to provide survelliance
    facilities over large terrains or buildings. As the space that needs to be observed increases, the
    data that needs to be processed increases too.
\ys{actually bullet 2 and 3 can be merged into one point, emphasizing that increasing environment size leads to constraints on shared resources}


\end{description}

%\item in such cases, the goal of the agent is to reason about the constraint and minimize uncertainty about future,
%\item since these systems observe highly dynamic scenes (ex ... multi-camera systems), it is critical to reason about long term consequences

In order for such a multi-camera system to perform optimally within real world limits there is a
need to operate its sensors in an intelligent way, so to extract the most information with the
minimum cost while observing the aforementioned constraints. In particular, since multi-camera
systems are often deployed in highly dynamic contexts where targets can move rapidly between tracked
locations, there is a need to predict effectively the long-term evolution of the state of the
environment. This can be done by implementing an agent which will reason about the system's
dynamics and employ available resources in the best way.

At the same time, the agent needs to act while facing direct uncertainty about the environment it is
trying to observe. Reasoning with uncertainty is not a trivial task, given that the agent cannot
know in advance what knowledge will be obtained by a specific actions. Given only a certain
\textit{belief} on the current state of the environment and probabilistic expectations over the
amounts of information that will be extracted by each available action, the agent needs to reason so
to maximize the information gathered.

%\item generally collecting information is not an means to an end , but in the example we gave above
% the aim of the agent is only to perceive the information. these type of tasks are called active
% perception task, define active prception. .....

The problem the agent faces falls under the \textit{Active Perception} \ys{cite} task. Active Perception can
be formalized as a task where a decision making entity called \textit{agent}, subject to resource
constraints, needs to take actions in order to reduce some form of uncertainty about a particular
target environment.  The agent's role in this task is purely observational: it will not influence
the overall evolution of the environment over time, but it can only selectively gather data to
maximally improve its knowledge.

\ys{this would be a good place to introduce the running example - surveillance task}

%\item pomdp provide a natural framework to model such problems.

Active Perception can be conveniently modeled through a powerful and flexible decision-theoretic
tool, called Partially Observable Markov Decision Process (POMDP) \cite{cit:pomdp}. We introduce
this framework in Section \ys{chapter} \ref{ref:background}. POMDPs provide a natural framework to model the
interactions between the agent and a partially observable, stochastic environment.  In the POMDP
framework the agent's objective is to maximize reward collection over time according to a predefined
reward function.

In this work \ys{we} approach the multi-camera tracking problem through the POMDP framework.  The main
reason why we use POMDPs is that they naturally allow for the computation of non-myopic strategies
for using the sensors. In fact, POMDPs are specifically structured as to allow this type of
planning. This means that the system will try to gather information in order to improve its expected
knowledge of the environment in the future, while reasoning about all possibilities. We argue that
while myopic strategies can sometimes suffice, many Active Perception problems require non-myopicity
to achieve optimality, and provide a proof by example in Appendix \ref{ref:appendix_proof}.

% \ys{proof is a big word, generally they are associated with theorems, which we are not presenting. You can say a appendix provides convincing example case.}
%
% Eugenio: It actually is a proof (by example). So I am correct in stating it to be a proof. I've
% added the "by example" part.

POMDPs are a widely explored topic in the decision theoretic literature, with known properties and
already available solution methods. We discuss some POMDPs solution methods in Chapter
\ref{ref:solutions}.
\ys{another option you have of mentioning where you are discuss what topic is at the end of the introduction, put a small paragraph, describing how the thesis is orgainized.}
In particular, our approach is based on online planning \ys{cite}. \ys{you can motivate online planning a little better by saying that we want to scale our solution to large system, hence we use online planning}. This type of approach is based on 
determining an optimal course of action applicable only to the situation that is currently faced by
the agent. This requires constant computations, but allows tacking problems orders of magnitude
bigger than other approaches.

In this work we extend one of the fastest online Monte Carlo approaches for POMDPs, Partially
Observable Monte Carlo Planning (POMCP) \cite{cit:pomcp}. This method approximates beliefs about the
world using particles, improving performance when the maintaining a complete belief over the state
of the world can become too computationally expensive.

The limitation of POMCP with respect to the Active Perception task is that the algorithm evaluates
actions with respect to rewards sampled from a generative model of the environment. Such model
cannot provide reward with respect to the current knowledge of the agent, and thus results useless
to compute a good policy for the agent. POMDPs where rewards are based on knowledge, rather than
state, have been examined in \cite{cit:rpomdp}. 
\ys{I dont understand this paragraph at all}

In this work approach the multi-camera tracking problem. The problem is modeled as a POMDP, and use
POMCP, an online planner, to find a good policy while dealing with potentially large models. We
extend POMCP and create a variant, which we named $\rho$-POMCP, which is able to deal with a reward
function based on knowledge rather than state. Our solution is able to refine estimates of action
rewards from the imperfect particle beliefs of POMCP. This allows to apply a fast online planner on
Active Perception tasks. We show empirical tests and performance of our method on a number of
problems.
