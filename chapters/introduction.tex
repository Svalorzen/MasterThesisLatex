\section{Active Perception}

In this work we approach a general class of problems which is referred to as the Active
Perception task. In this task a decision making entity called \textit{agent}, subject to resource
constraints, needs to take actions in order to reduce some form of uncertainty about a particular
target environment. The agent's role in this task is purely observational: it will not influence the
overall evolution of the environment over time, but it can only selectively gather data to maximally
improve its knowledge.

Active Perception covers essentially every possible use of sensors, both in physical and virtual
environments - in case of abstract data collection. Robotic perception, camera surveillance,
military reconnaissance, data collection in social networks and online retailer websites are all
examples of practical instances of the Active Perception task.

The fact that the agent is constrained is what makes the Active Perception task an interesting and
non-trivial problem to solve. The task thus focuses on maximizing performance within the limits
imposed on the agent: using available resources in the best possible way. The resource constraints
are most often found as instances of one or more of the following factors:

\begin{description}
\item[Sensor number and capabilities:] These prevent the agent from gathering data from specific
    locations or with particular features.
\item[Energy, bandwidth and communication:] These limit the amount of information the agent can
    gather at once and/or overall. Not only these resources have a direct cost for usage, but in
    many applications there are hard limits on how much a system is allowed to consume.
\item[Storage:] It is important to reduce the amount of non-informative data that is gathered by the
    sensors. Not everything is equally important. Being able to decide whether to gather data or not
    helps in the long run maintaining reasonable sized data logs requiring less storage space and
    easing possible future data searches.
\end{description}

Additionally, the agent needs to act while facing direct uncertainty about the environment it is
trying to observe. Reasoning with uncertainty is not a trivial task, given that the agent cannot
know in advance what knowledge will be obtained by a specific actions. The agent needs to select the
best action given only a certain belief on the current state of the environment.

In this work we present an autonomous system that tackles some of the challenges that characterize
the Active Perception task:

\begin{description}
\item[Knowledge Representation:] The system managing the sensors needs to be able to correctly
    maintain a representation of its currently available knowledge, as well as possibly missing
    information. This can get quite expensive computationally when the size of the environment is
    non-trivial.
\item[Reward Measure:] While knowledge representation is a problem in itself, the system also needs
    to be able to tell which knowledge is actually important, and what measure of uncertainty it
    should try to minimize. A popular target function to maximize, for example, is negative entropy.
\item[Scalability:] Real world Active Perception systems often have to deal with very sizable
    environments. It is important that deployable solutions can deal with such complexity.
    Additionally, they should able to adapt to possible changes in the environment - for example new
    sensor deployments - as painlessly as possible.
\item[Non-Myopicity:] We argue that an optimal Active Perception solution may need to take into
    account the way that the environment will evolve in the future. This requires the ability to
    forecast possible outcomes from the observation process in order to determine optimal sequences
    of actions over continued periods of time.
\end{description}

\section{Application}

The Active Perception task is a general abstraction that can be applied to multiple real world
problems. At the same time, focusing attention on a specific subproblem produces more realistic
tests where solutions can be evaluated and may also produce real world value. While keeping
generality of approach, in this work we focused our attention onto a particular set of Active
Perception tasks: multi-camera systems, which are themselves a sub-class of multi-sensor systems.

With the advent of cheap hardware, these last few years have brought an incredible widespread
adoption of multi-camera systems. These systems are used for surveillance, real-time tracking and
many other purposes. Deployment expenses of such systems have progressively decreased with the
advent of digital multiplexing, the Internet and new manufacturing techniques. At the same time the
cost for operating the cameras and storing/analyzing the incredible amount of data they provide has
been increasing ever since, also due to their increasing numbers.

Although automatic mechanisms that try to record only useful footage do already exist the cameras
have still the need to be turned on at all times. An example of this are motion detectors. A motion
triggered camera would write its data to disk only when motion appeared on the screen, even though
the camera would still be on when nothing was happening in front of it, to keep the motion detector
on.

In addition to the "always on" problem, new video processing techniques can now be applied to
captured footage, such as face recognition and person tracking, which require non-trivial amount of
computation and bandwidth to be run together with the cameras themselves. This incurs in significant
energy and hardware costs, since they have to be applied to all cameras. Communicating all camera
data and perform remote computations can be even \textit{more} expensive.

The challenges found in multi-camera systems are instantiations of the challenges in Active
Perceptions. In particular, we focus on the following:

\begin{description}
\item[Energy, bandwidth and communication:] These are the most common constraints for multi-camera
    systems. Such constraints can also be hard, meaning that there is really a maximum amount of
    energy or bandwidth available, and the agent should be able to perform its task nonetheless.
\item[Environment Size:] Environments tracked using cameras can be sizable, even ignoring the fact
    that the real world is continuous.
\item[Number of Targets:] Each tracked target contributes exponentially in increasing the complexity
    of the environment. In addition the number of targets to look for could be unknown in advance,
    which significantly complicates modeling the resulting environment.
\end{description}

%\section{Research Questions}
%
%Active Perception is an active field of research, and so many approaches to the problem have been
%experimented with. In particular, this work tries to give an answer to the following questions,
%which to our knowledge have not been answered yet:
%
%\begin{itemize}
%\item To what extent is a non-myopic solution necessary in order to achieve an optimal solution
%within the Active Perception problem?
%\item What are the trade-offs for using online Monte Carlo approximations in Active Perception with
%respect to other approaches?
%\end{itemize}

\section{Approach}

In this work we approach Active Perception by leveraging a powerful and flexible decision-theoretic
tool, called Partially Observable Markov Decision Process (POMDP) \cite{cit:pomdp}. We introduce
this framework in Section \ref{ref:background}. POMDPs allow to model the interactions between the
agent and a partially observable, stochastic environment.  In this framework, the agent's objective
is to maximize reward collection according to a predefined reward function.

The main reason why we use POMDPs is that they naturally allow for the computation of non-myopic
strategies for using the sensors. In fact, POMDPs are specifically structured as to allow this type
of planning. This means that the system will try to gather information in order to improve its
expected knowledge of the environment, while reasoning about all future possibilities. We argue that
while myopic strategies can sometimes suffice, many Active Perception problems require non-myopicity
to achieve optimality.

%In POMDPs, the agent tracks is current status via a \textit{belief}, which is a probability
%distribution over the possible states of the underlying world. This belief is used to compute
%possible outcomes for actions undertaken by the system, and to predict future rewards. This belief
%is updated depending on the observation that the system receives from the sensors, taking into
%account possible ambiguity (perceptual aliasing) that could result from imperfect knowledge.

POMDPs are a widely explored topic in the decision theoretic literature, with known properties
and already available solution methods, which allows us to stand on the shoulders of giants. We
discuss some POMDPs solution methods in Chapter \ref{ref:solutions}.

% One of the active topics of research in decision theory regards the efficient computation of POMDP
% policies. When dealing with non-trivial models, the time needed to compute an optimal policy and the
% space needed to store it increase exponentially with the complexity of the problem \cite{cit:pomdp}.
% This has led to the development of methods that allow for approximate solutions.

In particular, our approach is based on online planning. This type of approach is based on
determining an optimal course of action applicable only to the situation that is currently faced by
the agent. This is in contrast with offline planning, where the whole problem is tackled at once
before deployment and the resulting policy stored for future usage. This usually requires a
significant amount of time in advance, and computation of the policy must be performed every time
the underlying model changes.

On the other hand, online approaches can handle problems orders of magnitude bigger, since they do
not allocate resources to situations which are not relevant to the agent's situation. This ability
tackles one of the challenges we have mentioned: scalability. In addition, it is easier to
experiment with different models, as online methods do not require any pre-computation. An example
of the power of online methods are Monte Carlo methods, which have demonstrated incredible
performance in the Go board game \cite{cit:mcts}.

In this work we extend one of the fastest online Monte Carlo approaches for POMDPs, Partially
Observable Monte Carlo Planning (POMCP) \cite{cit:pomcp}. This method approximates beliefs about the
world using particles, improving performance when the maintaining a complete belief over the state
of the world can become too computationally expensive. This allows the method to scale onto problems
that other online approaches cannot handle.

The limitation of POMCP with respect to the Active Perception task is that rewards are computed
directly from the state of the world, rather than the current knowledge that the planner possesses.
POMDPs where rewards are based on knowledge, rather than state, have been examined in
\cite{cit:rpomdp}. Since POMCP is a sampling based method which does not perform true belief
updates, we cannot feed the real beliefs into the reward function.

In this work we present a modification of the POMCP algorithm, which we named $\rho$-POMCP, with the
following characteristics:

\begin{description}
    \item[Belief based reward function:] We modify the POMCP algorithm in order to evaluate actions
        based on a belief dependent reward function, rather than a state based reward function. This
        allows to approach the Active Perception problem by having a reward function taking into
        consideration knowledge rather than world state.
    \item[Estimate based rewards:] We modify the POMCP algorithm in order to backpropagate reward
        estimations. Normally sample rewards are progressively averaged into a final estimate, which
        only requires backpropagation of the rewards obtained during a sample trajectory. In
        $\rho$-POMCP an updated reward estimate does not average with previous estimations but
        completely overrides them, given that the uncertainty in a belief is deterministic.
        This requires a different backpropagation procedure.
    \item[Mean vs Max:] We test against two different functions used for value back-propagation within
        the lookup tree used by POMCP. The original algorithm computes the value of a particular
        belief as the weighted average over tried actions, instead that over the best available
        action. This was shown to be a good approach when the number of samples was low in
        \cite{cit:mcts}, but we argue that the processing power available to modern computers allows
        for the max function to be used more effectively.
\end{description}

In addition, $\rho$-POMCP is applicable to any POMDP where the reward function is based on knowledge
rather than states, and not only on the Active Perception task. This increases its utility, as it
can also be used in other classes of problems.

% This may go into experiments.. is our approach completely independent of application?
%
%In this work we consider the problem of tracking a person moving throughout an environment which is
%covered by a multi-camera system. We try to devise a centralized, intelligent method to
%automatically activate cameras by predicting the movement of the target within range of the cameras,
%and reducing as much as possible the uncertainty of their positions with limited resources. The idea
%is to select the cameras which will provide the maximum amount of information within the available
%resource limits. In a real world setting, such limits are generally defined as computational costs,
%energy requirements and communication costs.
%
%The camera selection problem is defined on already deployed systems, where camera positions and
%orientations are known, and not modifiable. The goal is to be able to select a subset $k$ of the $n$
%total cameras deployed, so as to maximize our knowledge of the state of the world; this is
%equivalent of minimizing the entropy of our knowledge. In our particular approach, we consider the
%case of tracking a single target using multiple sensors.
%
%In this work we are going to use passive sensors in the form of fixed cameras. These cameras can be
%seen as passive sensors since they only collect video information in a limited, non-interactive way.
%The idea is to actively utilize these sensors by using only a subset of them at any time, in order
%to reduce at the same time computational, energy and communication costs. We reduce energy by using
%less cameras at any one time, and we can reduce computational and communication costs by reducing
%the amount of data that needs to be processed concurrently.

\subsection{Chapter Overview}
Throughout Chapter \ref{ref:background} we introduce the background theory and models that support
our approach, and define the terms we use within this work. In addition, we introduce the already
existing techniques that led to our particular approach on the problem. In Chapter
\ref{ref:approach} we discuss the unique problems that we faced in this work, and our proposed
solutions. In Chapter \ref{ref:relwork} we discuss work that has already been done in the literature
and we relate it to our approach. In Chapter \ref{ref:experiments} we show our experiments and
results. Finally, in Chapter \ref{ref:conclusion} we discuss our results and offer our insights.

