%\item multi-sensor systems are everywhere. ex ....

With the advent of cheap hardware, these last few years have brought an incredible widespread
adoption of multi-camera systems. These systems are used for surveillance, real-time tracking and
many other purposes. Deployment expenses of such systems have progressively decreased with the
advent of digital multiplexing, the Internet and new manufacturing techniques. At the same time the
cost for operating the cameras and storing/analyzing the incredible amount of data they provide has
been increasing ever since. This often results in a need to operate multi-camera systems within
tight constraints, caused by both budget and technological limitations. Some such constraints can
be:

%\item they have resource constraints sometimes , ex ....

\begin{description}

\item[Limited Sensor Range:] Generally it is impossible for sensors to perceive simultaneously the
    whole environment in which they are located. This can result from a limited number of available
    sensors and/or from sensors having physical limits in what they can record. This constraint
    makes environments partially observable.

\item[Energy, Bandwidth and Communication Restraints:] Gathering and processing information consumes
    resources. Not only energy and bandwidth are expensive, but in many applications there are hard
    limits on their availability, both for what can be used at any given time and for the overall
    resource usage. This constraint forces information gathering from a subset of available sensors.

\item[Environment Size:] Camera systems are often deployed in order to provide surveillance
    facilities over large terrains or buildings. As the space that needs to be observed increases, the
    data that needs to be processed increases too. This constraint also limits the number of sensors
    that can be used at any one time, although the dependency here is more on the type and precision
    of the analysis performed on the data rather than a tight limit.

\end{description}

%\item in such cases, the goal of the agent is to reason about the constraint and minimize uncertainty about future,
%\item since these systems observe highly dynamic scenes (ex ... multi-camera systems), it is critical to reason about long term consequences

In order for such a multi-camera system to perform optimally within real world limits there is a
need to operate its sensors in an intelligent way, so to extract the most information with the
minimum cost while observing the aforementioned constraints. In particular, since multi-camera
systems are often deployed in highly dynamic contexts where targets can move rapidly between tracked
locations, there is a need to predict effectively the long-term evolution of the state of the
environment. This can be done by implementing an agent which will reason about the system's
dynamics and employ available resources in the best way.

At the same time, the agent needs to act while facing direct uncertainty about the environment it is
trying to observe. Reasoning with uncertainty is not a trivial task, given that the agent cannot
know in advance what knowledge will be obtained by a specific actions. Given only a certain
\textit{belief} on the current state of the environment and probabilistic expectations over the
amounts of information that will be extracted by each available action, the agent needs to reason so
to maximize the information gathered in the future.

%\item generally collecting information is not an means to an end , but in the example we gave above
% the aim of the agent is only to perceive the information. these type of tasks are called active
% perception task, define active perception. .....

The problem the agent faces falls under the \textit{Active Perception} \cite{cit:relworkspaan}
\cite{cit:relworkspaancoop} \cite{cit:relworksatsangi} task.  Active Perception can be formalized as
a task where a decision making entity called \textit{agent}, subject to resource constraints, needs
to take actions in order to reduce some form of uncertainty about a particular target environment.
The agent's role in this task is purely observational: it will not influence the overall evolution
of the environment over time, but it can only selectively gather data to maximally improve its
knowledge.

% \ys{this would be a good place to introduce the running example - surveillance task}

In this work we approach a surveillance task as an Active Perception problem: our goal is to produce
an agent system which will be able to track as best as possible a target moving through a partially
observable environment. The agent will need to exploit its available resources to determine the
target position over time. The agent will need to reason about the movements of the target even when
not observed, and use available cameras to observe the environment to refine as much as possible its
estimates over time.

%\item pomdp provide a natural framework to model such problems.

Active Perception tasks can be conveniently modeled through a powerful and flexible
decision-theoretic tool, called Partially Observable Markov Decision Process (POMDP)
\cite{cit:pomdp}. POMDPs provide a natural framework to model the interactions between the agent and
a partially observable, stochastic environment.  In the POMDP framework the agent's objective is to
maximize reward collection over time according to a predefined reward function.

POMDPs naturally allow for the computation of non-myopic strategies when reasoning over time is
required, as they are specifically structured as to allow this type of planning. This allows the
agent to gather information in order to improve its expected knowledge of the environment in the
future, while reasoning about all possibilities. We argue that while myopic strategies can sometimes
suffice, many Active Perception problems require non-myopicity to achieve optimality, and provide a
proof by example in Appendix \ref{ref:appendix_proof}.

% \ys{proof is a big word, generally they are associated with theorems, which we are not presenting. You can say a appendix provides convincing example case.}
%
% Eugenio: It actually is a proof (by example). So I am correct in stating it to be a proof. I've
% added the "by example" part.

% I think you should start by saying that you solve the surveillance problem (hopefully you introduce
% the surveillance problem somewhere in introduction) using online planning.
% ----------------
% I think you should motivate online planning
% here on the idea that as the size of the system increases, it is difficult to use offline methods

POMDPs are a widely explored topic in the decision theoretic literature, with known properties and
already available solution methods. In particular, our approach is based on online planning
\cite{cit:relworkonlineall}. This type of approach is based on determining an optimal course of
action applicable only to the situation that is currently faced by the agent, ignoring the others.
Online planning requires reasoning continuously over time, but it is simpler to perform as the size
of a problem increases than offline methods, which tackle the whole problem at once beforehand.

% You use MCTS to solve the problem. ... we use MCTS because of better performance

In this work we approach the surveillance problem by extending one of the fastest online Monte Carlo
planners for POMDPs, Partially Observable Monte Carlo Planning (POMCP) \cite{cit:pomcp}. This
method approximates beliefs about the world using particles, improving performance when the
maintaining a complete belief over the state of the world can become too computationally expensive.

The limitation of POMCP with respect to the Active Perception task is that the algorithm evaluates
actions using rewards which depend on the state of the environment. Unfortunately, such evaluation
cannot take into account the knowledge of the agent, and thus results useless to compute a good
policy for the agent when it needs to maximize its knowledge. POMDPs where rewards are based on
knowledge, rather than state, have been examined in \cite{cit:rpomdp}.

% You propose a new variant of MCTS, suitable for belief-dependent rewards leading to better
% estimates/performance.

We extend POMCP and create a variant, which we named $\rho$-POMCP, which is able to deal with a
reward function based on knowledge rather than state. Our solution is able to refine estimates of
action rewards from the imperfect particle beliefs of POMCP. This allows to apply a fast online
planner on Active Perception tasks.

% Finally we illustrate empirical success of our method.

We show empirical tests and performance of our method on a number of problems which show the
effectiveness of our method with limited resources and its ability to scale with the size of the
problem with respect to already available methods.

