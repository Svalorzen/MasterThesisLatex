\section{$\rho$-POMCP}

As discussed, the POMCP algorithm can't be applied directly to $\rho$-POMDPs. In this Section we
present a modification of the algorithm POMCP which is able to deal with a belief-based reward
function in POMDPs. We discuss in more details why POMCP cannot be applied directly to $\rho$-POMDPs
in Section \ref{ref:rewestimation}.

We call our POMCP variant $\rho$-POMCP. An important fact to note is that $\rho$-POMCP is able to deal
with any $\rho$-POMDP, and not only with Active Perception tasks. For example, this algorithm can be
applied to problems where the agent can influence the state of the world, and also (with minor
modifications to reintroduce state dependent rewards) when the reward function is both dependent on
states and belief.

\subsection{Non-PWLC Value Function}

Negative entropy is not a PWLC function. This means that the value function of a $\rho$-POMDP which
uses it as reward function will not be PWLC too \cite{cit:rpomdp}. This is a problem for exact
offline planning since usual solving techniques rely on the PWLC property in order to detect beliefs
where the currently computed optimal policy is not actually optimal. Once such a belief point is
discovered (called \textit{witness point}), the optimal policy is updated to be optimal in that
belief too.

This problem is approached generally via approximation. With $\rho$-POMCP, we directly approximate
rewards for the possible futures of the current situation, without the need to solve a POMDP
completely for any possible beliefs. Thus, witness points do not need to be discovered, and we can
ignore the PWLC property.

\subsection{Reward Estimation}\label{ref:rewestimation}

The main challenge when applying POMCP to a $\rho$-POMDP is that the algorithm can only extract
reward information from a generative model of the environment. Since a belief is an abstraction
created by the agent, a generative model containing a belief-dependent reward function cannot exist.

A possible approximate solution involves converting the belief-dependent reward function into a
state-based reward function. The main drawback is that this results in an exponentially increasing
number of actions the more accuracy is required, with consequent increase in the space of search
that needs to be considered by the solution process \cite{cit:rpomdp}.

Instead, in our approach we estimate the immediate reward for a specific action-observation
transition by using maximum-likelihood estimates of the belief extracted from the particle beliefs
POMCP generates. However, since particle beliefs are updated constantly, computing a direct estimate
of every belief reached during each sample episode would excessively hamper performance. We solved
this problem differently for estimating max of belief rewards and negative entropy rewards.

For estimation of max of belief, $\rho$-POMCP simply keeps track of the number of particles in the
particle belief $N(b)$, the number of particles for each particle type $N(s)$ and the most common
type $max_s$. At each particle $k$ insertion $N(k)$ is compared with $N(max_s)$. Whichever is higher
determines the new most common particle, either $k$ or the old $max_s$. Finally, the new max of
belief is computed using maximum likelihood as $N(max_s)/N(b)$.

\begin{algorithm}[H]
    \caption{Max of Belief Reward Estimation}
    \SetKwFunction{update}{Update Estimate}

    \SetKwProg{main}{Algorithm}{}{}
    \main{\update}{
        \KwData{Particle Belief b, Particle s}
\nl     N(s) = N(s) + 1\;
\nl     \If{N(s) $>$ N(max\_s)}{
\nl         max\_s = s\;
        }
\nl     $\rho(b)$ = $\frac{N(max\_s)}{N(b)}$\;
    }
\end{algorithm}

For estimation of entropy, the situation is not quite so straightforward. Entropy estimation from
discrete samples is bound to be biased, and there is no way to remove this bias completely
\cite{cit:badentropy}. On the other hand, techniques to decrease bias are computationally heavy and
rely on usage of additional complex distributions to account for bias \cite{cit:entropyfixes}.

In practice bias becomes small as long as the number of samples is high. Thus, we kept our
estimation algorithm as simple as possible in order to maximize the number of samples that can be
done in any given time. We compute negative entropy as a summation of terms, one for each type of
particle. We thus approximate the entropy, whose formula is:

\[ -H(b) = \sum_s p(s) \log p(s) \]

With the following computation:

\[ -H(b) \approx \sum_s \frac{N(s)}{N} \log \frac{N(s)}{N} \]

Where $N$ is the total count for all states encountered. $\rho$-POMCP stores internally both the
latest negative entropy estimate $-H(b)$, and each of the separate terms that compose it. At each
particle $k$ insertion in a particle belief, we simply remove from our previous full estimate the
relevant term for $k$, recompute the term and add it again. This approach is not mathematically
correct as, in theory, all terms would need to be updated at each insertion, but in practice it
turns out not to be a problem as long as each type of particle $s$ is added with relative frequency.

\begin{algorithm}[H]
    \caption{Negative Entropy Reward Estimation}
    \SetKwFunction{update}{Update Estimate}

    \SetKwProg{main}{Algorithm}{}{}
    \main{\update}{
        \KwData{Particle Belief b, Particle Type s}
\nl     $\rho(b)$ = $\rho(b)$ - $\rho(s)$\;
\nl     N(s) = N(s) + 1\;
\nl     p = $\frac{N(s)}{N(b)}$\;
\nl     $\rho(s)$ = p $\cdot$ log(p)\;
\nl     $\rho(b)$ = $\rho(b)$ + $\rho(s)$\;
    }

\end{algorithm}

Such an approach is guaranteed to converge to the true entropy in the limit, when the number of
samples tends to infinite. This maintains the POMCP property that leads to an optimal solution in
the limit.

% \ys{can you cite something to support this?}
%
% Eugenio: Not directly, aside from the law of large numbers.. since in the limit each state will be
% visited an infinite number of times, the approximation for each \rho(s) will converge to its true
% value, thus making \rho(b) converge too.. but I don't have anything specific to write this yet. If
% you want I can just remove the claim I guess..

\subsection{Value Backup}

A second problem is dependent on how returns are calculated in POMCP. $Q(b,a)$ is computed as the
sum of two different factors: the immediate reward $\rho(b,a)$ plus a discounted term representing
the expected return from then on.

\[ Q_h(b,a) = \rho(b,a) + \gamma \cdot \sum_{o\in \Omega} O(o|b,a) V_{h-1}(b') \]

POMCP tries to approximate the value of $\rho(b,a)$ as computed in Equation \ref{rhoeq} by averaging
all results from $R(s,a)$ done using particles extracted from the all beliefs and actions
encountered during sample episodes.

\[ Q_h(b,a) \approx \frac{\sum_{s \in S} n_s R(s,a)}{N} + \gamma \cdot \frac{\sum_{o\in \Omega} n_o f_o}{N} \]

Where $N$ is the number of times that action $a$ has been taken in belief $b$, $n_s$ the number of
times state $s$ has been sampled from $b$ when taking action $a$, $n_o$ is the number of times that
observation $o$ has been seen from $b$ and $a$ and $f_o$ is the average of all returns sampled after
observation $o$. As with MCTS, as long as the better actions are chosen more often than the others,
this procedure will converge to the true expected return.

However, in our case, the reward function is structured in the following way:

\[ \rho(b,a) = \sum_{o\in \Omega} O(o | b, a) \rho(b') \]

This is because both max of belief and negative entropy rewards (represented by $\rho(b')$) can be
only extracted from a belief, and do not depend on actions. Thus, the value for a belief-action pair
becomes:

\[ Q_h(b,a) = \sum_{o\in \Omega} O(o | b,a) \rho(b') + \gamma\sum_{o\in\Omega} O(o|b,a) V_{h-1}(b') \]

The problem is that our method of estimation for $\rho(b')$ does not allow a direct back-propagation
of each $\rho$ estimation back up the chain through $V$, as it happened in POMCP via samples. This
means that whenever a new estimate for any given $\rho(b)$ is computed, $\rho$-POMCP needs to
substitute the previous estimate all the way up the chain, and cannot be simply averaged in as it
happened in POMCP.

We solved this problem by keeping the average mechanism present in POMCP, with the difference that
we average together fake datapoints built specifically to replace previous estimates. For
example, suppose we have a particular estimate $r$ for $\rho(b, a)$.

\[ \rho(b,a) \approx r \]

We can decompose this into all our estimates for all beliefs reachable from $b$ and $a$:

\[ \rho(b,a) \approx \frac{\sum_{o\in\Omega} n_o r_o}{N} \]

Suppose now we experience $b$ and $a$ again, and obtain observation $\tilde{o}$. We update our new
reward estimate for the new belief into $r'_{\tilde{o}}$. If we averaged normally, we would get:

\[ \rho(b,a) \approx \frac{ n_{\tilde{o}} r_{\tilde{o}} + r'_{\tilde{o}} +
\sum_{o \neq \tilde{o} \in \Omega} n_o r_o}{N+1} \]

This is not what we want, however. Instead, suppose we average in $n_{\tilde{o}}(r'_{\tilde{o}} -
r_{\tilde{o}}) + r'_{\tilde{o}}$:

\[ \rho(b,a) \approx \frac{ n_{\tilde{o}} r_{\tilde{o}} + n_{\tilde{o}}(r'_{\tilde{o}} -
r_{\tilde{o}}) + r'_{\tilde{o}} +
\sum_{o \neq \tilde{o} \in \Omega} n_o r_o}{N+1} \]

\[ \rho(b,a) \approx \frac{ n_{\tilde{o}} r_{\tilde{o}} + n_{\tilde{o}}r'_{\tilde{o}} -
        n_{\tilde{o}} r_{\tilde{o}} + r'_{\tilde{o}} +
\sum_{o \neq \tilde{o} \in \Omega} n_o r_o}{N+1} \]

\[ \rho(b,a) \approx \frac{ ( n_{\tilde{o}}+1) r'_{\tilde{o}} +
\sum_{o \neq \tilde{o} \in \Omega} n_o r_o}{N+1} \]

Which is what we wanted. Similarly, we can create fake reward points to backup in the tree so that
any $V(b)$ can be substituted with the more up-to-date $V'(b)$. The value backed up would be:

\[ N ( V'(b) - V(b) ) + V'(b) \]

\subsection{Max vs Mean}

The POMCP algorithm averages together all sampled rewards, and depends on UCT to let the tree
converge to the true optimum. This works since UCT samples the best actions infinitely more often in
the limit, which lets the final estimated value for it converge to the true value. However, this
procedure needs to sample many more times to make up for the fact that, at first, results for
suboptimal actions are averaged into estimates too.

POMCP inherits this behavior from the original MCTS algorithm. However, in the original paper from
2006 a number of different approaches to merge together backed up values were tried
\cite{cit:mcts}. The authors decided to stick with using the mean operator to backup values since,
with the computational power available at the time, it gave better and more consistent results.
However, the authors also demonstrated that if enough samples are available, using a max operator
over actions actually improves performance.

We implemented both backup methods for $\rho$-POMCP in order to determine which one would perform
best under different conditions.

\subsection{Pseudocode}

Here we describe the full $\rho$-POMCP algorithm in more detail.

\begin{algorithm}[H]
    \caption{$\rho$-Partially Observable Monte-Carlo Planning}
\begin{multicols}{2}
    \SetKwFunction{pomcp}{$\rho$-POMCP}
    \SetKwFunction{simulate}{Simulate}

    \SetKwProg{main}{Algorithm}{}{}
    \main{\pomcp{b}}{
        \KwData{Belief b}
\nl     \While{enough time is available}{
\nl         s $\sim$ b\;
\nl         \simulate{\O, s, 0}\;
        }
\nl     \KwRet $\arg\max_a V(a)$\;
    }

    \setcounter{AlgoLine}{0}
    \SetKwProg{simul}{Procedure}{}{}
    \simul{\simulate{h, s, d}}{
        \KwData{History h, State s, Depth d}
\nl     N($h$) = N($h$) + 1\;
\nl     select action $a$ with UCT\;
\nl     ($s', o$) $\sim$ G($s,a$)\;
\nl     \eIf{T($h,a,o$) is \O}{
\nl         initialize T($h,a,o$)\;
\nl         newNode = true\;
        }{
\nl         newNode = false\;
        }
\nl     B($h,a,o$) = B($h,a,o$) $\bigcup \;\{ s' \}$\;
\nl     \eIf{$d < $ horizon and newNode == false}{
\nl         r = \simulate{$(h,a,o), s', d+1$}\;
        }{
\nl         N($h,a,o$) = N($h,a,o$) + 1\;
\nl         r = $\rho(h,a,o)$\;
        }
\nl     N($h,a$) = N($h,a$) + 1\;
\nl     V($h,a$) = V($h,a$) + $\frac{r - V(h,a)}{N(h,a)}$\;
\nl     oldV = V($h$)\;
\nl     V($h$) = $\rho(h)$ + $\gamma \cdot \max_{a'} V(h,a')$\;
\nl     \KwRet N($h$) * ( V($h$) - oldV ) + V($h$);
    }
\end{multicols}
\end{algorithm}

$\rho$-POMCP starts off as POMCP, by simulating episodes from the root of the tree.

Starting from the root, $\rho$-POMCP samples a single state $s$ from the root's particle belief.
Then, using UCT, it selects a state $a$, and uses it with $s$ to sample a new state $s'$, a reward
$r$ and an observation $o$. The root child representing the sequence $(a,o)$ is selected, or if it
did not exist, it is created. The algorithm then updates the particle belief of the child before
deciding whether to continue to descend the tree. While descending, POMCP appends $s'$ to the
particle belief of the selected child node. Then, it repeats the action selection and sampling
process using $s'$ as its $s$. This repeats until the algorithm is unable to descend further into
the tree, at which point the future reward is approximated as simply the reward of the last
selected child's belief.

The algorithm then updates the value of all actions taken during the descent, while backing up fake
datapoints to update all old reward estimates up in the tree.

Notice that $\rho$-POMCP lacks a rollout procedure. This is because $\rho$-POMCP has no way to
approximate rewards when outside the tree, and so skips random policy rollouts altogether.

\section{Multi Person}

One of the main limiting factors for Monte-Carlo online methods is the branching factor of the
tackled problem. While MCTS, POMCP and $\rho$-POMCP can deal with incredibly large state and
observation spaces, they cannot deal easily with a problem that leads to a vast number of branches
to be explored. This is because each new branch significantly increases the number of sample
episodes that need to be performed in order to obtain an accurate estimate for the best action.

The main branching factor in the problem we are considering, which is multi-sensor systems, is the
number of targets to be tracked. This is because each target moves independently, and thus from a
given state there are many more possible states reachable. Thus, the branching factor for a problem
is exponential in the number of targets.

In order to avoid dealing with this problem, we setup a system where we run concurrently many
$\rho$-POMCP instances. Each solver computes an individual policy for a single target only and
receives individual observations. This allows each solver to work on a relatively small tree,
without exploding the number of branches. In addition, this allows to work with a variable number of
targets, since it possible to dynamically adjust the number of online planners running at any time,
depending on the number of targets present in the environment.

We then estimate the final action to be performed by merging the separate results of all the
solvers.  From each solver we then obtain a value estimate for each available action; the final
action is then the one that maximizes the sum of all values across solvers. In order to obtain
consistent evaluations of all root actions so that we can compare them between solvers, we disable
UCT for the first depth level in the tree, and instead sample equally from each action.
