\section{Partially Observable Markov Decision Process}\label{ref:pomdp}
\sectionmark{POMDP}

In order to optimally perform a task within an environment we need to jointly model the dynamics of
the environment and the task objectives. The modeling framework we are going to use in this work
stems from the \textit{Markov Decision Process} (MDP), named after Andrey Markov.

MDPs structure reasoning around a probabilistic automaton. The agent \textit{acts} upon the
environment, which is in a particular \textit{state}. As a result, the environment
\textit{transitions} stochastically to another state, as defined by a transition function. During
the transition, the agent receives a certain \textit{reward} representing its achievements
toward the task at hand.

The \textit{Partially Observable MDP} (POMDP) is an extension of such a framework, where the agent
cannot see the current state of the world. In POMDPs, the agent receives an \textit{observation}
dependent on the underlying state during each transition, but which does not uniquely disambiguate
it. These are used to keep a statistic over the states the environment could be in at any given
timestep.

Non-myopic strategies can be naturally extracted from MDPs and POMDPs, which is one of their main
advantages over competing approaches.

A POMDP can be defined formally as a tuple

\[ POMDP = <S,A,T,R,\Omega,O,h> \]

The elements of the tuple are called state space($S$), action space($A$), transition function($T$),
reward function($R$), observation space($\Omega$), observation function($O$) and horizon($h$). We
introduce these terms in the following Sections.

\subsection{Agent}

The agent is at the core of the decision process. Its objective is to maximize the reward obtained
by acting in the environment while reasoning about the state uncertainties. In doing so it will
accomplish the task that is encoded within the POMDP model.

\subsection{Environment}

The environment represents all that is not the agent. The environment is described in terms of its
possible configurations.  Each configuration, called \textit{state}, represents a unique
arrangement of the world at any one time. For example, in chess, the environment can be completely
described by the positions of all pieces on the game board. The full set of states is denoted
\textit{state space}, and is usually referred to as $S$. This is the first element of the POMDP
tuple.

\[ S = \{s_0, s_1, s_2, ..., s_{|S|}\} \]

Note that we assume that all sets presented in this work to be finite, unless where specifically
noted.

In the environment, transitions between a state and another state are discrete and instantaneous.
Each transition happens within a single \textit{timestep}, which represents the atomic unit of
change within the environment.

\subsection{State}

As we said above, our \ys{the} environment is described through states, which all together form the
\textit{state space} of the problem. Each state is unique, meaning that it represents a unique
circumstance of the environment, different in some particular way from all others. At the same time,
a state ignores all that is not relevant to the decision making progress. In a chess game, it does
not matter whether at the time of playing it's raining, but only the position of the pieces on the
board.

An extremely important property that each state needs to have is called the \textit{Markov}
property. This property requires that the evolution of the environment depends solely on the
information encoded in its current state, and nothing else. This can also be seen as saying that the
environment has no memory, since keeping track of previous states has no use for its future
evolution. The Markov property is required in order to allow the agent to reason only about the
current state, without the need to know its history.

While it can happen that a particular environment can evolve in a time-sensitive manner, it can be
easily shown that all such environments can be modeled in a Markovian way by simply adding more
features into the state-space definition \cite{cit:boutilier}.


\subsection{Action}

The agent interacts with the environment through actions. In the POMDP framework, each action
simultaneously fulfills two purposes: to influence the evolution of the environment, and to gather
information regarding the state of the environment.

The full set of actions available to the agent is called the \textit{action space}, or otherwise
referred to as $A$. This is the second element of the POMDP tuple. It is usually assumed that all
actions are available to the agent at any timestep.

\[ A = \{ a_0, a_1, a_2, ..., a_{|A|} \} \]

\subsection{Observation}

In a POMDP the agent does not have \ys{direct} access to the current state of the environment. The agent instead
receives, at each timestep, an \textit{observation}, which depends on the action that the agent
performed and on the state reached by the environment after the action. Observations generally do
not allow unambiguous distinction between states, but they restrict the amount of possibilities.

All observations are, as are states and actions, grouped in a set, $\Omega$, where usually $|\Omega| <
|S|$. This is the fifth element of the POMDP tuple:
\ys{no need to mention the tuple number repeatedly, Also there is no need to mention $|\Omega| <
|S|$, this is not necessarily true, is it used somewhere in the thesis?}
\[ \Omega = \{ o_0, o_1, o_2, ..., o_{|\Omega|} \} \]

\subsection{Transition Function}

In a POMDP, the evolution dynamics of the environment are encoded in a function, called
\textit{transition function}. This function encodes the consequences of actions with respect to the
environment states.

In particular the transition function is a function

\[ T: S\times A \times S \rightarrow [0,1] \]

that specifies the probability for any transition in the environment, depending on the actions that
the agent can take. In other words, it specifies the probability that by applying a particular
action in a particular state, we end up into a second state. Using this function, the agent can
determine the best way to reach a particular state from any other state. Another way to see this
function is to look at it as representing a conditional probability function:

\[ T(s, a, s') = Pr(s' | s, a) \]

Where $Pr(s'|s,a)$ is the probability of the environment transitioning to $s'$ from $s$ when the
agent performs action $a$. By using this notation, we can show formally what the Markov property is
about. This property holds when

\[ Pr(s_{t+1} | s_{0}, a_{0}, s_{1}, a_{1}, ..., s_{t}, a_{t} ) = Pr(s_{t+1} | s_t, a_t ) \hspace{1cm} \forall s \in S \]
\ys{mention here that $s_{t}$, $t$ denotes the time step, earlier you defined $s_{1}$ as first state in the set $S$}

\subsection{Reward Function}

In a POMDP we encode the goals that we wish the agent to achieve as a \textit{reward function}.
This function is once again \ys{once again?} defined upon states and actions, but \ys{but?} can take values within the real
domain:

\[ R: S\times A\times S \rightarrow \mathbb{R} \]
\[ R(s, a, s') \in \mathbb{R} \]

The reward function represents the level of appeal that a particular situation or outcome represents
in the modeled task. For example, one could allocate a certain reward for attempting to walk, and an
even greater reward for successful efforts. On the other hand, one could allocate a negative reward
for failed attempts. All these would influence the final behavior of the agent.

In these terms, all possible achievements and penalties have a weight and can be compared to each
other, in order to allow the agent to determine which goal \ys{which goal sounds like agent has more than one goal, I suppose you mean "which action"} to pursue depending on circumstances.

Note that the reward function specifies a reward for each single transition the agent encounters,
independently from the current timestep. Often most of the transitions have no particular reward,
but the agent still needs to consider them all when computing its policy.
\ys{this statement is unclear, every $<s,a>$ pair has a reward , may be 0}

\subsection{Observation Function}

The POMDP framework \ys{framework does not model anything, it provides a way to model observation function} models the way the agent is able to receive information from observations using
an \textit{observation function}. It is a function

\[ O : S' \times A \times O \rightarrow [0,1] \]

representing the probability of receiving a certain observation upon reaching a given state $s'$
after taking action $a$.

\[ O(s', a, o) = Pr(o | s', a) \]

Note that the state is the one of \textit{arrival}, not the one where the action is performed.



\subsection{Horizon}

In the POMDP framework the agent tries to maximize its \textit{expected return} within an
\textit{episode}: the expected sum of all rewards obtained within a single try at the task.

The length of a specific episode is determined by a parameter called \textit{horizon}. An horizon of
1 timestep generates what is generally known as a \textit{myopic policy}. Such a policy acts in a
locally optimal way, hoping that in doing so it will act in an optimal way globally. As the horizon
increases the final policy takes into account more and more possible outcomes, becoming more complex
but, depending on the task, more performant overall. We can describe the goal function of a POMDP
with finite horizon $h$ as:

\[ \mathbb{E}[\text{return}] = \mathbb{E} \left [ \sum_{t=0}^{h} R(s_t, a_t, s_{t+1}) \right ] \]

\subsection{History and Belief}

The observation function does not allow the agent to disambiguate between states unambiguously, or
the problem would convert directly to an MDP. Instead, what happens is called \textit{perceptual
aliasing}: each obtained observation is compatible with a subset of $S$. For example, in a room
where half the walls are white, a robot seeing white would not know for certain where it is looking.
At the same time, it would still be better off than not knowing anything at all.

The agent must reason about which of the possible underlying states of the environment are
compatible with all previously obtained observations in order to plan its moves.

Unfortunately, this implies that the agent is now unable to reason about the present. While the
environment states maintain their Markov property, the agent cannot observe them. Instead it needs
to store internally its whole history of actions and observations in order to keep track of which
states are possible at any point in an episode. This long list of actions and observations is called
\textit{history}.

An alternative way of maintaining the history is keeping a \textit{belief} over the current state of
the environment. Such a belief is simply a probability vector in $|S|$ dimensions, where each
element denotes the probability that the environment is in a particular state. The belief is then \ys{then?} a
\textit{sufficient statistic} of the state of the environment.

At each new timestep, the agent's belief needs to be updated with the new knowledge gained by the
agent. The belief can be updated with the following formula:

\[ b'(s') = \frac{O(s', a, o)\sum_{s\in S}T(s,a,s')b(s)}{Pr(o|a,b)} \]

The denominator can be treated as a normalizing factor that makes $b'$ correctly sum up to 1, since
it is a probability vector.

\subsection{Belief MDP}

The belief allows for a reinterpretation of the POMDP framework. The belief can in fact be used in
lieu of a state, since it is possible to compute new transition and reward functions that depend on
beliefs rather than states. This transforms automatically the POMDP into a \textit{belief MDP}
\cite{cit:pomdp}, where the belief is now the state of the environment.

The belief MDP is created by generating a new transition function $\tau$ and reward function $\rho$
as follows:

\[ \tau(b,a,b') = \sum_{o\in \Omega} Pr(b' | b, a, o) \sum_{s\in S} O(s,a,o) b(s) \]

Where

\[Pr(b' | b, a, o) = \left\{
  \begin{array}{lr}
    1 \hspace{.6cm}\text{if the belief update with arguments $b,a,o$ returns $b'$}\\
    0 \hspace{.6cm}\text{otherwise}
  \end{array}
\right.
\]

Similarly:

\[ \rho(b,a) = \sum_{s\in S} R(s,a) b(s) \]

Although this transformation allows us to forget the partial observability, which is now embedded in
$\tau$, we have now introduced a new problem: a continuous state space. Although continuous MDPs are
very hard to solve generally, the belief MDP can be approached due to its specific properties.

\subsection{Policy}

At this point we have introduced all elements that compose a POMDP. Here we talk about the solution
itself, how it looks like and what it represents, while postponing the discussion about solving
algorithms for Section \ref{ref:solutions} onward.
\ys{this intro is not necessary at all!}

A solution for a POMDP is called a \textit{policy}. This policy specifies a sequence of actions to
be taken by the agent, depending on the beliefs the agent will face while acting in the model. The
policy's decisions take into account the chosen horizon for the problem. We can define a policy
$\pi$ as a function, that given an horizon, a belief and action returns the probability of the action
being chosen:
\ys{"This policy $\dots$  chosen horizon for the problem" is not necessary, can do without it}
\[ \pi : H \times B \times A \rightarrow [0,1] \]
\[ \pi(h, b, a) \in [0,1] \]

We can also define it as a function, taking a belief and an horizon, returning a distribution over
actions:

\[ a \sim \pi(h, b) \]

\subsection{Value Function}

In order to know whether the agent is performing optimally within a given POMDP, we need a way to measure
the performance of any given policy. This is done via the \textit{value function}.
\ys{the first two statements are not necessary}
The value function maps a policy with its performance in terms of expected return from every
possible belief the agent may face and a particular horizon. Thus, every belief has a certain
\textit{value} under a certain policy and horizon. Formally then we can define the value of a belief
as:

\[ V^\pi_{h}(b) = \mathbb{E}^\pi_h \left [\text{return} \mid b_0 = b \right ] \]
\[ = \mathbb{E} \left [\sum_{t=0}^{h} \rho(b_{t}, a_t, b_{t+1}) \right ]
    \hspace{0.5cm}\text{where }b_0 = b \text{ and }a_t \sim \pi(h-t, b_t) \]

This value is very important because it lets us compare and evaluate policies. This is because a
better policy does have a equal or higher value for every possible belief than an inferior policy
\cite{cit:suttonbarto}.

A value function can be expressed recursively with what is called \ys{what is called?} the \textit{Bellman equation}.
The value of a state can be computed as the expected reward that the agent can obtain in the
next timestep, plus a value which represents what is going to happen in the future.

\[ V^{\pi}_{h}(b) = \sum_a \pi(h, b, a) \sum_{b'} \tau(b, a, b') \left [ \rho(b, a, b') +
V^{\pi}_{h-1}(b') \right ] \]

Since the value function of a POMDP is a function dependent on beliefs, it is defined on a simplex
space of $|S|-1$ dimensions, which is continuous. The property that makes a belief MDP tractable is
that its value function is \textit{piecewise linear and convex} (PWLC). This means that it is
``determined by a set of hyperplanes, the value at given belief point being that of the highest
hyperplane'' \cite{cit:rpomdp}. This makes it possible to solve a belief MDP exactly
\cite{cit:pomdp}.

In general, when we are looking for a solution for a POMDP we are looking for a policy $\pi^*$, called
\textit{optimal policy}, that maximizes the value over all beliefs:

\[ V^{\pi^*}_h = \max_\pi V^{\pi}_h \]

\section{$\rho$POMDP}

As we have already mentioned, in a belief MDP the reward is defined in terms of the belief, called
\textit{belief dependent reward}. $\rho$ is then defined as a linear function over the original
POMDP reward function and the belief space:

\[ \rho(b,a) = \sum_{s\in S} R(s,a) b(s) \]

However, in some cases the reward function is solely dependent on the information the agent has on
the current state of the world, rather than on the underlying state of the world itself. In such
situations the original POMDP reward function becomes useless, and as such our definition of $\rho$.
For example, in the case of surveillance, there is no inherent reward for the agent when the
environment is in a state rather than another. Instead, the agent actions should maximize the
knowledge that the agent has of the world.

In such cases the $\rho$ function is defined independently, and is not tied to states anymore.
Unfortunately, this means giving up on the PWLC property of the value function, which was the only
way in which it was possible to solve a POMDP in the first place. While it has been proved in
\cite{cit:rpomdp} that as long as the newly defined $\rho$ function is PWLC then the value function
will be PWLC as well, functions such as entropy are not PWLC in the first place. We discuss how we
overcame this problem in Section \ref{ref:approach}.

\ys{I am confused, if you are using $\rho$POMDP in your experiments or not. If you are directly computing the belief based reward then you are not using the $\rho$POMDP. If you are using $\rho$ POMDP, then you need to mention here how $rho$ POMDP solves the problem of belief dependent reward. We have not solved that problem, $\rho$ POMDP has! }
