\section{Planning vs Reinforcement Learning}\label{ref:solutions}

While often the possible states, actions and observations of a particular environment are easy to
define, it does not always happen that the transition, observation and reward functions are known in
advance.

When they are known, the solving process is called \textit{planning}, and consists in efficiently
looking through all possible transitions the agent could experience, and finding the best path
through them in order to maximize the return.

On the other hand, when they are not known, \textit{reinforcement learning} is used. In this case the
focus is on gathering knowledge about the environment while \textit{at the same time} exploiting that
knowledge to obtain the maximum return. Reinforcement learning methods can range from approximating
the unknown functions from experience and then planning, to approximating the optimal value function
of the model and using it to obtain a policy, or even directly approximating the optimal policy.

In this work we assume we are either given a model, or a model is learned in advance, and that this
model is used as a base for planning.

\section{Offline vs Online}

An optimal policy can be applied at virtually no computational cost with the knowledge that the
behavior is constantly optimal. Computing such a solution in advance and using it later is called
\textit{offline planning}. The main disadvantage of offline planning is its very heavy upfront cost:
it needs to store all possible policies to be applied to any reachable belief at any horizon. This
is expensive not only in terms of space, but even more on computation, since any non-trivial POMDP
has an exponential explosion in the number of possible histories that an agent can experience and
each one of them has to be considered in order to evaluate the final policy \cite{cit:pomdp}. In
addition, any small change in the original model requires a full recomputation of the optimal
policy, which limits the amount of tuning that can be feasibly done on the model.

Offline planning methods generally leverage the Bellman equation and the recursive nature of the
value function; they plan in reverse temporal order, from the last steps the agent will take to the
first ones. This is because a policy for a certain horizon $h$ is always composed by a set of
actions to apply in certain beliefs which then leads to a policy for horizon of $h-1$. Thus, the
final policy needed is computed incrementally. All possible state, actions and observations are
considered, so as to provide the correct action in every possible situation.

In order to avoid the computational cost of offline methods, a new venue of approach was invented
called \textit{online planning}. In online planning the agent continuously plans for its current
belief, and nothing else. This requires constant computations, but at the same time makes possible
to tackle problems orders of magnitude bigger than what is possible with offline planning, and allow
for flexibility in updating the model, since there is no need to recompute a complete solution. In
addition, many online planning methods are \textit{any time}: this means that they can run for
whatever time is available, and they are always able to provide some sort of solution. The more time
is available, the more refined the solution will be. This is in contrast with offline methods, that
need to run to completion or otherwise nothing is gained.

In online planning computations go forward in time. From the current state possible futures are
expanded in a tree of possibilities and rewards. In the end, the best action for the current state
is picked, the environment is advanced one step, and the process repeats again.

In this work we chose to use an online approach, given the flexibility and power provided over
offline techniques, and because one of our goals was to scale to real-world scenarios, which are
generally intractable with offline approaches. An additional advantage which we will explore later
is that online planners are able to deal with non-PWLC value functions, which is a challenge when
using negative entropy as a reward function.

\section{Monte Carlo Algorithms}

At the time of writing, one of the most promising ways of planning online is by using \textit{Monte
Carlo algorithms}. Monte Carlo algorithms are a class of randomized algorithms which use random
processes in order to compute their output. The output will be correct within a certain probability,
which trades off with their speed, deterministic running time and anytime properties.

In the MDP/POMDP frameworks Monte Carlo algorithms only need results from samples of the
environment; as in, transition and reward results sampled from the model's distribution using
particular state-action pairs as inputs. Such a model is also called a \textit{generative model}. A
generative model $G$ fed with state $s$ and action $a$ would return a new state $s'$ and reward $r$
respecting the model probabilities:

\[ s', r \sim G(s, a) \]

An advantage of using such a model is that it many cases it is possible to obtain samples without
having the whole transition function for the model in an explicit form. The samples obtained can
then be used to approximate average returns, which in turns can be used to obtain an optimal policy.

\section{Monte Carlo Tree Search}

The most widely known algorithm using Monte Carlo techniques is called \textit{Monte Carlo Tree
Search} (MCTS) \cite{cit:mcts}. This algorithm is an MDP online planner which has received high
praise for its contributions in the development of highly performant board game AIs.

MCTS was originally devised as a method to approach the board game Go. The algorithm runs Monte
Carlo simulations from the current state, which is the input, and progressively builds
a tree of outcomes.

\begin{algorithm}
\begin{multicols}{2}
    \caption{Monte Carlo Tree Search}
    \SetKwFunction{mcts}{MCTS}
    \SetKwFunction{simulate}{Simulate}
    \SetKwFunction{rollout}{Rollout}

    \SetKwProg{main}{Algorithm}{}{}
    \main{\mcts{s}}{
        \KwData{State s}
\nl     \While{enough time is available}{
\nl         \simulate{\O, s, 0}\;
        }
\nl     \KwRet most valued action in tree root\;
    }

    \setcounter{AlgoLine}{0}
    \SetKwProg{myproc}{Procedure}{}{}
    \myproc{\rollout{s, d}}{
        \KwData{State s, Depth d}
\nl     r = 0\;
\nl     discount = 1\;
\nl     \While{$d <$ horizon}{
\nl         ($s', r'$) $\sim$ G($s$, random action)\;
\nl         r = r + discount * $r'$\;
\nl         discount = discount * $\gamma$\;
\nl         $s$ = $s'$\;
        }
\nl     \KwRet r\;
    }

    \setcounter{AlgoLine}{0}
    \SetKwProg{simul}{Procedure}{}{}
    \simul{\simulate{h, s, d}}{
        \KwData{History h, State s, Depth d}
\nl     select action $a$\;
\nl     ($s', r$) $\sim$ G($s,a$)\;
\nl     \If{$d < $ horizon}{
\nl         \eIf{T($h,a,s'$) is not \O}{
\nl             futureRew = \simulate{$(h,a,s'), s', d+1$}\;
            }{
\nl             initialize T($h,a,s'$)\;
\nl             N($h,a$) = V($h,a$) = 0\;
\nl             futureRew = \rollout{s', d+1}\;
            }
\nl         r = r + $\gamma$ * futureRew\;
            }
\nl     N($h,a$) = N($h,a$) + 1\;
\nl     V($h,a$) += $\frac{r-V(h,a)}{N(h,a)}$\;
\nl     \KwRet r\;
    }

\end{multicols}
\end{algorithm}

The MCTS tree of outcomes starts out as a single root. This root represents the current state of the
agent. From this root MCTS carries out as many sample episodes as possible. After each sample
episode, the MCTS tree is expanded, keeping track of all possible futures experienced by MCTS.

Each sample episode can be divided into two phases: in the first the algorithm descends the
known tree, while in the second the algorithm simulates outside it.

Descending the tree is done recursively in a very simple fashion. Starting from the root node $s$, the
algorithm selects an action $a$, and uses both to produce a reward $r$ and a next state $s'$. The
action is selected probabilistically, with more promising actions selected more often.  The root
children are then searched for a node representing the sequence $(a,s')$. If one is found, the
algorithm descends one level in the tree, and repeats the action selection and sampling process,
this time using the last $s'$ as its $s$.  This repeats until the algorithm is unable to descend
further into the tree.

Once the bottom of the tree is reached, the algorithm adds a single child node to the reached leaf.
The node represents the $(a,s')$ from the last sample. Once this is done, MCTS enters its
second phase.

During the second phase the algorithm continues to simulate the episode, but it does not use the
tree anymore. Actions are selected using a \textit{rollout policy}, which is generally uniformly
random. This continues until a terminal state or the selected horizon is reached. This concludes the
sample episode.

Once a sample episode ends, all the reward collected during both phases is backpropagated up to the
tree root. This is used to update the values of all the actions taken during the first phase of the
algorithm.

At this point, a new sample episode is started from the root of the tree. This process is repeated
until there is time left.

A nice property of MCTS is that it is guaranteed to converge to the optimal policy as long as the
most promising actions are given a higher probability of being chosen than the others. This is
because in the long run the value averages will converge to the values of the actions taken most
often.  In addition, since it builds a tree, part of the work done can be reused on the next time
step. Once a move is tried in the real world, the tree root can be moved to the new state perceived,
while the rest of the tree is discarded.

\section{Upper Confidence Bounds for Trees}

A technique which has recently gained popularity to exactly determine which actions should be chosen
within the tree and which ratio should be used between promising actions and the rest is called
\textit{Upper Confidence Bounds for Trees} (UCT) \cite{cit:uct}. UCT tries to empirically derive a
variance estimate from the data, sorting actions by their upper bounds on value.

The upper bound estimate is composed of two parts: the currently estimated value of the action $v$,
and the estimated UCT variance. Thus the value that UCT assigns at a given action $a$ is:

\[ UCT(a) = v + c*\sqrt{\frac{\log{N}}{n}} \]

Where $N$ is the number of times the node in the tree currently being considered has been visited,
and $n$ is the number of times action $a$ has already been tried from the node. $c$ is a constant
which is dependent on the problem, which defines the amplitude of the variance, which needs to be in
tune with the magnitude of the rewards of the MDP. This number is generally tuned manually, and is
around the maximum return that can be obtained by the agent.

Actions are then selected greedily with respect to the UCT value.

\section[]{Partially Observable Monte-Carlo Planning%
\sectionmark{POMCP}}
\sectionmark{POMCP}

\textit{Partially Observable Monte-Carlo Planning}(POMCP) \cite{cit:pomcp} is one of the
state-of-the-art online solvers available today. This algorithm incorporates the advantages of MCTS
while adding support for belief states and observations.

The trouble of naively applying MCTS to a belief MDP extracted from a POMDP is its necessity to
sample directly from state to state. While this can be a cheap operation in MDPs, requiring
samples from relatively simple distributions, computing a new belief given a previous belief and
observation in a POMDP, called \textit{belief update}, can be a very expensive operation. Recall that a belief
update can be performed using the following formula:

\[ b'(s') = \frac{O(s', a, o)\sum_{s\in S}T(s,a,s')b(s)}{Pr(o|a,b)} \]

For a full belief, this requires $O(|S|^2)$ operations, which for POMDPs with a large state space
can result in an extremely expensive operation. Since MCTS requires a very high number of samples in
order to provide useful answers, each sample needs to be performed in as little time as possible,
and such expensive operations are thus not acceptable.

POMCP solves this problem by representing beliefs as particle vectors containing a certain amount of
state particles. Such a \textit{particle belief} can be created by repeatedly sampling a belief, and
gathering all samples in a vector - not a set.

\begin{algorithm}
    \caption{Partially Observable Monte-Carlo Planning}
\begin{multicols}{2}
    \SetKwFunction{pomcp}{POMCP}
    \SetKwFunction{simulate}{Simulate}
    \SetKwFunction{rollout}{Rollout}

    \SetKwProg{main}{Algorithm}{}{}
    \main{\pomcp{b}}{
        \KwData{Belief b}
\nl     \While{enough time is available}{
\nl         s $\sim$ b\;
\nl         \simulate{\O, s, 0}\;
        }
\nl     \KwRet most valued action in tree root\;
    }

    \setcounter{AlgoLine}{0}
    \SetKwProg{myproc}{Procedure}{}{}
    \myproc{\rollout{s, d}}{
        \KwData{State s, Depth d}
\nl     r = 0\;
\nl     discount = 1\;
\nl     \While{$d <$ horizon}{
\nl         (s', r') $\sim$ G(s, random action)\;
\nl         r = r + discount * r'\;
\nl         discount = discount * $\gamma$\;
\nl         s = s'\;
        }
\nl     \KwRet r\;
    }

    \setcounter{AlgoLine}{0}
    \SetKwProg{simul}{Procedure}{}{}
    \simul{\simulate{h, s, d}}{
        \KwData{History h, State s, Depth d}
\nl     select action $a$ with UCT\;
\nl     ($s', o, r$) $\sim$ G($s,a$)\;
\nl     \If{$d < $ horizon}{
\nl         \eIf{T($h,a,o$) is not \O}{
\nl             futureRew = \simulate{$(h,a,o), s', d+1$}\;
            }{
\nl             initialize T($h,a,o$)\;
\nl             N($h,a$) = V($h,a$) = 0\;
\nl             futureRew = \rollout{s', d+1}\;
            }
\nl         r = r + $\gamma$ * futureRew\;
            }
\nl     B(h) = B(h) $\bigcup \;\{ s' \}$\;
\nl     N($h,a$) = N($h,a$) + 1\;
\nl     V($h,a$) += $\frac{r-V(h,a)}{N(h,a)}$\;
\nl     \KwRet r\;
    }

\end{multicols}
\end{algorithm}

POMCP starts its execution in the root of the tree, which represents the current particle
belief of the agent. POMCP, as MCTS, is again composed of two phases: tree descent and outside tree
simulation. The procedures for both are very similar to the MCTS originals.

Starting from the root, POMCP samples a single state $s$ from the root's particle belief. Then,
using UCT, it selects a state $a$, and uses it with $s$ to sample a new state $s'$, a reward $r$ and
an observation $o$. The root children are then searched for a node representing the sequence
$(a,o)$. If one is found, the algorithm descends one level into the tree. While descending, POMCP
appends $s'$ to the particle belief of the selected child node. Then, it repeats the action
selection and sampling process using $s'$ as its $s$. This repeats until the algorithm is unable to
descend further into the tree.

The adding of particles to lower level particle beliefs is the way POMCP updates beliefs, without
the $O(|S|^2)$ complexity.

Once the bottom of the tree is reached, the algorithm adds a single child node to the reached leaf.
The node represents the $(a,o)$ from the last sample. It also adds to the new node's particle belief
its first particle, the final $s'$. Once this is done, POMCP enters its second phase.

During the second phase the algorithm continues to simulate the episode, but it does not use the
tree anymore. Actions are selected using a \textit{rollout policy}, and observations sampled are
completely discarded. This continues until a terminal state or the selected horizon is reached. This
concludes the sample episode.

Once a sample episode ends, all the reward collected during both phases is backpropagated up to the
tree root. This is used to update the values of all the actions taken during the first phase of the
algorithm.

At this point, a new sample episode is started from the root of the tree. This process is repeated
until there is time left.
